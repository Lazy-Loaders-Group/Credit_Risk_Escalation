# ğŸš€ Quick Start Guide - Credit Risk Escalation System

**Last Updated:** November 5, 2025  
**Estimated Time:** 2-3 hours for complete execution

---

## âœ… Prerequisites Checklist

Before starting, ensure you have:

- [ ] Python 3.8+ installed
- [ ] Git installed (to clone repository)
- [ ] At least 4GB free RAM
- [ ] At least 2GB free disk space
- [ ] Internet connection (for downloading packages)

---

## ğŸ“¥ Step 1: Get the Project (5 minutes)

### Option A: Clone from GitHub
```bash
# Open terminal and run:
git clone https://github.com/Lazy-Loaders-Group/Credit_Risk_Escalation.git
cd Credit_Risk_Escalation
```

### Option B: Download ZIP
```bash
# 1. Download ZIP from GitHub
# 2. Extract to your desired location
# 3. Navigate to folder
cd path/to/Credit_Risk_Escalation
```

---

## ğŸ”§ Step 2: Environment Setup (10 minutes)

### For macOS/Linux:
```bash
# 1. Create virtual environment
python3 -m venv uom_venv

# 2. Activate virtual environment
source uom_venv/bin/activate

# 3. Upgrade pip
pip install --upgrade pip

# 4. Install all dependencies
pip install -r requirements.txt

# 5. Verify installation
python -c "import pandas, sklearn, xgboost, shap; print('âœ… All packages installed successfully!')"
```

### For Windows:
```cmd
REM 1. Create virtual environment
python -m venv uom_venv

REM 2. Activate virtual environment
uom_venv\Scripts\activate

REM 3. Upgrade pip
python -m pip install --upgrade pip

REM 4. Install all dependencies
pip install -r requirements.txt

REM 5. Verify installation
python -c "import pandas, sklearn, xgboost, shap; print('âœ… All packages installed successfully!')"
```

### Expected Output:
```
âœ… All packages installed successfully!
```

**âœ… If you see this, you're ready to go!**

---

## ğŸ¯ Step 3: Quick Test (2 minutes)

Verify everything is working:

```bash
# Make sure virtual environment is activated (you should see "(uom_venv)" in terminal)
# Run quick test
python -c "
import pandas as pd
import os

# Check if dataset exists
data_file = 'data/raw/LC_loans_granting_model_dataset.csv'
if os.path.exists(data_file):
    df = pd.read_csv(data_file, nrows=5)
    print(f'âœ… Dataset found: {len(df)} rows loaded (sample)')
    print(f'âœ… Columns: {list(df.columns[:5])}...')
    print('âœ… Ready to start!')
else:
    print('âŒ Dataset not found. Please ensure data/raw/LC_loans_granting_model_dataset.csv exists')
"
```

### Expected Output:
```
âœ… Dataset found: 5 rows loaded (sample)
âœ… Columns: ['FICO.Score', 'Employment.Length', 'Home.Ownership', 'Annual.Income', 'Verification.Status']...
âœ… Ready to start!
```

---

## ğŸš€ Step 4: Execute the Project (2-3 hours)

### Launch Jupyter Notebook

```bash
# Make sure virtual environment is activated
jupyter notebook
```

This will open Jupyter in your browser at `http://localhost:8888`

---

### ğŸ““ Execution Order (IMPORTANT!)

Execute notebooks **in this exact order**:

#### **1ï¸âƒ£ Notebook 1: Data Exploration** (Already Complete âœ…)
```
File: notebooks/01_data_exploration_executed.ipynb
Status: âœ… Already executed with results
Action: Just review the outputs (no need to re-run)
Time: 5 minutes (review only)
```

**What to look for:**
- Dataset size: 1,048,575 rows, 15 columns
- Target distribution: 80% no default, 20% default
- Key features identified: FICO score, DTI, Income, etc.

---

#### **2ï¸âƒ£ Notebook 2: Baseline Model Development** ğŸƒâ€â™‚ï¸ **RUN THIS FIRST**
```
File: notebooks/02_baseline_model.ipynb
Action: Kernel â†’ Restart & Run All
Time: 20-30 minutes
```

**Step-by-step:**
1. Open `02_baseline_model.ipynb` in Jupyter
2. Click **Kernel** â†’ **Restart & Run All**
3. Wait for all cells to complete (you'll see `[*]` change to `[number]`)
4. Check final output shows:
   - âœ… 3 models trained (Logistic Regression, Random Forest, XGBoost)
   - âœ… Best model selected (usually XGBoost)
   - âœ… AUC-ROC > 0.75
   - âœ… Models saved to `results/models/`

**Common outputs during execution:**
```
Processing data...
Training Logistic Regression... Done
Training Random Forest... Done
Training XGBoost... Done
Hyperparameter tuning... (this takes 10-15 minutes)
Best parameters found: {...}
Final test accuracy: 0.793
AUC-ROC: 0.823
âœ… Models saved to results/models/
```

**Troubleshooting:**
- If memory error: Close other programs
- If "kernel died": Restart kernel and run cells one by one
- If slow: Normal for large dataset, be patient

---

#### **3ï¸âƒ£ Notebook 3: Uncertainty Quantification** ğŸƒâ€â™‚ï¸ **RUN SECOND**
```
File: notebooks/03_uncertainty_quantification.ipynb
Action: Kernel â†’ Restart & Run All
Time: 40-60 minutes (trains 30 models!)
```

**Step-by-step:**
1. Open `03_uncertainty_quantification.ipynb`
2. Click **Kernel** â†’ **Restart & Run All**
3. **Go get coffee â˜•** - this takes 40-60 minutes
4. Check final output shows:
   - âœ… 30-model bootstrap ensemble trained
   - âœ… Uncertainty calibration validated
   - âœ… Uncertainty vs error correlation > 0.3
   - âœ… Ensemble saved to `results/models/`

**Progress indicators:**
```
Training bootstrap model 1/30... Done
Training bootstrap model 2/30... Done
...
Training bootstrap model 30/30... Done
Calculating uncertainties...
Uncertainty-error correlation: 0.324
âœ… Ensemble saved!
```

**This is the slowest notebook - be patient!**

---

#### **4ï¸âƒ£ Notebook 4: Escalation System** ğŸƒâ€â™‚ï¸ **RUN THIRD**
```
File: notebooks/04_escalation_system.ipynb
Action: Kernel â†’ Restart & Run All
Time: 15-20 minutes
```

**Step-by-step:**
1. Open `04_escalation_system.ipynb`
2. Click **Kernel** â†’ **Restart & Run All**
3. Watch as it optimizes escalation thresholds
4. Check final output shows:
   - âœ… Optimal thresholds found
   - âœ… Automation rate: 70-85%
   - âœ… Accuracy on automated: >85%
   - âœ… Cost savings calculated
   - âœ… System saved to `results/models/`

**Expected final results:**
```
Optimal Configuration:
- Uncertainty threshold: 0.173
- Confidence threshold: 0.725
- Automation rate: 78.3%
- Automated accuracy: 88.76%
- Cost savings: 20.9%
âœ… Escalation system saved!
```

---

#### **5ï¸âƒ£ Notebook 5: Comprehensive Evaluation** ğŸƒâ€â™‚ï¸ **RUN LAST**
```
File: notebooks/05_comprehensive_evaluation.ipynb
Action: Kernel â†’ Restart & Run All
Time: 30-40 minutes
```

**Step-by-step:**
1. Open `05_comprehensive_evaluation.ipynb`
2. Click **Kernel** â†’ **Restart & Run All**
3. Wait for SHAP analysis (slow but worth it!)
4. Check final output shows:
   - âœ… SHAP feature importance plots
   - âœ… Ablation study comparison
   - âœ… Business impact summary
   - âœ… All visualizations saved

**Progress indicators:**
```
Running SHAP analysis... (10-15 minutes)
Generating feature importance plots...
Running ablation study...
- Baseline only: 79.3% accuracy
- Ensemble only: 81.2% accuracy  
- Complete system: 88.76% accuracy
âœ… Evaluation complete!
```

---

## ğŸ“Š Step 5: Verify Results (10 minutes)

After all notebooks complete, verify you have these files:

### Check Generated Models:
```bash
ls -lh results/models/

# Expected files:
# - preprocessor.pkl
# - baseline_model_best.pkl
# - xgboost_best.pkl
# - xgboost_calibrated.pkl
# - bootstrap_ensemble.pkl
# - escalation_system.pkl
```

### Check Generated Figures:
```bash
ls results/figures/

# Expected files (15+ plots):
# - target_distribution.png
# - correlation_matrix.png
# - feature_importance.png
# - roc_curve.png
# - calibration_curve.png
# - uncertainty_distribution.png
# - escalation_tradeoff.png
# - shap_summary.png
# ... and more
```

### View Final Report:
```bash
# Open in VS Code or text editor
code results/reports/FINAL_PROJECT_REPORT.md

# Or view in terminal
cat results/reports/FINAL_PROJECT_REPORT.md | less
```

---

## ğŸ¯ Step 6: Understanding Your Results

### Key Metrics to Check:

Open `PROGRESS.md` to see all results:
```bash
code PROGRESS.md
```

**Look for these success criteria:**

| Metric | Target | Your Result | Status |
|--------|--------|-------------|--------|
| Baseline AUC-ROC | >0.75 | Check notebook 2 | â¬œ |
| Automation Rate | 70-85% | Check notebook 4 | â¬œ |
| Automated Accuracy | >85% | Check notebook 4 | â¬œ |
| Cost Savings | Positive | Check notebook 4 | â¬œ |
| SHAP Analysis | Complete | Check notebook 5 | â¬œ |

**âœ… If all metrics meet targets, your project is successful!**

---

## ğŸ” Step 7: Using the Trained System

### Make Predictions on New Data:

```python
# In a new Python script or notebook:
import pandas as pd
import joblib

# 1. Load the complete system
preprocessor = joblib.load('results/models/preprocessor.pkl')
ensemble = joblib.load('results/models/bootstrap_ensemble.pkl')
escalation_system = joblib.load('results/models/escalation_system.pkl')

# 2. Load new loan applications
new_data = pd.read_csv('new_applications.csv')

# 3. Preprocess
X_new = preprocessor.transform(new_data)

# 4. Get predictions with uncertainty
predictions, uncertainties = ensemble.predict_with_uncertainty(X_new)

# 5. Make escalation decisions
decisions = escalation_system.make_decisions(predictions, uncertainties)

# 6. Review results
results = pd.DataFrame({
    'application_id': new_data['id'],
    'default_probability': predictions,
    'uncertainty': uncertainties,
    'decision': decisions['action'],  # 'approve', 'reject', or 'escalate'
    'confidence': decisions['confidence']  # 'low', 'medium', 'high'
})

# 7. Separate into automated vs escalated
automated = results[results['decision'].isin(['approve', 'reject'])]
escalated = results[results['decision'] == 'escalate']

print(f"Total applications: {len(results)}")
print(f"Automated: {len(automated)} ({len(automated)/len(results)*100:.1f}%)")
print(f"Escalated to humans: {len(escalated)} ({len(escalated)/len(results)*100:.1f}%)")
```

---

## ğŸ› ï¸ Troubleshooting Common Issues

### Issue 1: "ModuleNotFoundError"
```bash
# Solution: Activate virtual environment
source uom_venv/bin/activate  # macOS/Linux
# or
uom_venv\Scripts\activate  # Windows

# Then reinstall packages
pip install -r requirements.txt
```

---

### Issue 2: "Kernel died" in Jupyter
```bash
# Solution: Increase memory or reduce ensemble size

# Edit notebook 3, find this line:
n_models = 30

# Change to:
n_models = 10  # Use fewer models for testing

# Or close other programs to free up RAM
```

---

### Issue 3: "FileNotFoundError: data/raw/..."
```bash
# Solution: Ensure dataset is in correct location
ls data/raw/LC_loans_granting_model_dataset.csv

# If missing, check if file exists elsewhere:
find . -name "*.csv" -type f

# Move it to correct location if found
```

---

### Issue 4: Notebooks take too long
```bash
# Solution: Use smaller sample for testing

# In notebook 2, add this after loading data:
df = df.sample(n=100000, random_state=42)  # Use 100K instead of 1M

# Note: Results will differ with smaller sample
```

---

### Issue 5: "GridSearchCV is too slow"
```bash
# Solution: Reduce parameter grid

# In notebook 2, find GridSearchCV section
# Reduce parameters:
param_grid = {
    'n_estimators': [100, 200],      # Reduced from [100, 200, 300]
    'max_depth': [6, 8],              # Reduced from [4, 6, 8, 10]
    'learning_rate': [0.05, 0.1]      # Reduced from [0.01, 0.05, 0.1]
}
```

---

### Issue 6: SHAP analysis crashes
```bash
# Solution: Reduce sample size

# In notebook 5, find SHAP section
# Change:
sample_size = 1000  # to
sample_size = 100   # Much faster, still meaningful
```

---

## âš¡ Quick Commands Reference

```bash
# Activate environment
source uom_venv/bin/activate  # macOS/Linux
uom_venv\Scripts\activate     # Windows

# Start Jupyter
jupyter notebook

# Check Python version
python --version

# List installed packages
pip list

# Update a package
pip install --upgrade package_name

# Deactivate environment
deactivate

# View logs if errors occur
tail -f ~/.jupyter/jupyter_notebook_config.py
```

---

## ğŸ“ Project Structure Reference

```
Credit_Risk_Escalation/
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ raw/                    # Original dataset (don't modify!)
â”‚   â”œâ”€â”€ processed/              # Cleaned data (auto-generated)
â”‚   â””â”€â”€ splits/                 # Train/val/test (auto-generated)
â”‚
â”œâ”€â”€ ğŸ“ notebooks/               # Execute in order: 01 â†’ 02 â†’ 03 â†’ 04 â†’ 05
â”‚   â”œâ”€â”€ 01_data_exploration_executed.ipynb  âœ… (review only)
â”‚   â”œâ”€â”€ 02_baseline_model.ipynb             ğŸƒ (run 1st)
â”‚   â”œâ”€â”€ 03_uncertainty_quantification.ipynb ğŸƒ (run 2nd - slowest!)
â”‚   â”œâ”€â”€ 04_escalation_system.ipynb          ğŸƒ (run 3rd)
â”‚   â””â”€â”€ 05_comprehensive_evaluation.ipynb   ğŸƒ (run 4th)
â”‚
â”œâ”€â”€ ğŸ“ src/                     # Python modules (imported by notebooks)
â”‚   â”œâ”€â”€ data_preprocessing.py   # Data cleaning & feature engineering
â”‚   â”œâ”€â”€ uncertainty_quantification.py  # Bootstrap ensemble
â”‚   â””â”€â”€ escalation_system.py    # Intelligent escalation logic
â”‚
â”œâ”€â”€ ğŸ“ results/                 # Auto-generated outputs
â”‚   â”œâ”€â”€ figures/                # Visualizations (15+ plots)
â”‚   â”œâ”€â”€ models/                 # Saved models (.pkl files)
â”‚   â””â”€â”€ reports/                # Analysis reports
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt         # Python dependencies
â”œâ”€â”€ ğŸ“„ QUICKSTART.md            # This file!
â”œâ”€â”€ ğŸ“„ PROGRESS.md              # Detailed project status
â””â”€â”€ ğŸ“„ README.md                # Project overview
```

---

## ğŸ“ Understanding the Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YOUR EXECUTION FLOW                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ DATA EXPLORATION (Notebook 1) âœ… Already Done
   â”œâ”€ Load 1M+ loan records
   â”œâ”€ Analyze distributions
   â”œâ”€ Identify patterns
   â””â”€ Generate 8 visualizations

2ï¸âƒ£ BASELINE MODEL (Notebook 2) ğŸƒ 20-30 min
   â”œâ”€ Preprocess data
   â”œâ”€ Handle class imbalance (SMOTE)
   â”œâ”€ Train 3 models
   â”œâ”€ Tune hyperparameters (GridSearch)
   â”œâ”€ Calibrate probabilities
   â””â”€ Save best model (XGBoost)

3ï¸âƒ£ UNCERTAINTY (Notebook 3) ğŸƒ 40-60 min
   â”œâ”€ Create 30 bootstrap samples
   â”œâ”€ Train 30 models
   â”œâ”€ Calculate prediction variance
   â”œâ”€ Validate uncertainty calibration
   â””â”€ Save ensemble

4ï¸âƒ£ ESCALATION (Notebook 4) ğŸƒ 15-20 min
   â”œâ”€ Load ensemble predictions
   â”œâ”€ Define cost functions
   â”œâ”€ Optimize thresholds (225 configurations)
   â”œâ”€ Evaluate automation vs accuracy
   â””â”€ Save escalation system

5ï¸âƒ£ EVALUATION (Notebook 5) ğŸƒ 30-40 min
   â”œâ”€ Run SHAP analysis (explainability)
   â”œâ”€ Ablation study (3 configurations)
   â”œâ”€ Business impact analysis
   â”œâ”€ Generate final visualizations
   â””â”€ Create production checklist

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FINAL DELIVERABLE                        â”‚
â”‚  Complete AI system that automates 78% of loan decisions    â”‚
â”‚  with 89% accuracy while escalating uncertain cases to      â”‚
â”‚  humans - saving 21% in operational costs!                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Success Checklist

Mark off as you complete each step:

### Setup Phase:
- [ ] Repository cloned/downloaded
- [ ] Virtual environment created
- [ ] Dependencies installed
- [ ] Dataset verified

### Execution Phase:
- [ ] Notebook 1 reviewed (already executed)
- [ ] Notebook 2 executed successfully (20-30 min)
- [ ] Notebook 3 executed successfully (40-60 min)
- [ ] Notebook 4 executed successfully (15-20 min)
- [ ] Notebook 5 executed successfully (30-40 min)

### Verification Phase:
- [ ] All models saved in `results/models/`
- [ ] All figures generated in `results/figures/`
- [ ] Metrics meet success criteria
- [ ] PROGRESS.md shows all phases complete

### Understanding Phase:
- [ ] Read FINAL_PROJECT_REPORT.md
- [ ] Understand automation rate (78.3%)
- [ ] Understand accuracy improvement (79% â†’ 89%)
- [ ] Understand cost savings (20.9%)
- [ ] Know how to use system on new data

---

## ğŸ¯ Expected Timeline

| Task | Duration | Can Work Offline? |
|------|----------|-------------------|
| Setup environment | 10-15 min | âŒ (needs internet) |
| Run notebook 2 | 20-30 min | âœ… |
| Run notebook 3 | 40-60 min | âœ… |
| Run notebook 4 | 15-20 min | âœ… |
| Run notebook 5 | 30-40 min | âœ… |
| Review results | 15-20 min | âœ… |
| **TOTAL** | **2-3 hours** | Mostly offline |

**ğŸ’¡ Tip:** Start the execution before lunch/break. Notebook 3 takes the longest (40-60 min) - perfect time for a coffee break! â˜•

---

## ğŸš€ Ready to Start?

Copy-paste these commands to begin:

```bash
# 1. Navigate to project
cd path/to/Credit_Risk_Escalation

# 2. Activate environment
source uom_venv/bin/activate

# 3. Verify setup
python -c "import pandas, sklearn, xgboost, shap; print('âœ… Ready!')"

# 4. Launch Jupyter
jupyter notebook

# 5. Open notebooks/02_baseline_model.ipynb
# 6. Click "Kernel" â†’ "Restart & Run All"
# 7. Wait for completion (20-30 min)
# 8. Repeat for notebooks 03, 04, 05
```

---

## ğŸ“ Need Help?

### Check These Resources:
1. **PROGRESS.md** - Current status and all metrics
2. **FINAL_PROJECT_REPORT.md** - Detailed explanations
3. **Notebook outputs** - Error messages and logs
4. **This file** - Troubleshooting section above

### Common Questions:

**Q: Can I run notebooks out of order?**  
A: âŒ No! They depend on each other (2â†’3â†’4â†’5)

**Q: What if my results differ slightly?**  
A: âœ… Normal! Random seeds cause small variations (~1-2%)

**Q: Can I run this on Google Colab?**  
A: âœ… Yes! Upload notebooks and install requirements

**Q: How much does this cost?**  
A: ğŸ†“ Free! All packages are open-source

**Q: Can I use this commercially?**  
A: âš–ï¸ Check data license - code is open for learning

---

## ğŸ‰ Congratulations!

Once all notebooks complete successfully, you'll have:

âœ… **5 trained ML models** (Logistic Regression, Random Forest, XGBoost, Ensemble, Escalation)  
âœ… **30-model uncertainty ensemble** (Bootstrap sampling)  
âœ… **Optimized escalation system** (Cost-benefit balanced)  
âœ… **15+ publication-quality visualizations**  
âœ… **Complete explainability analysis** (SHAP values)  
âœ… **Production-ready system** (78% automation, 89% accuracy)

### Business Value:
- ğŸ’° **20.9% cost reduction** ($678 per 210K applications)
- âš¡ **78.3% automation rate** (only 21.7% need human review)
- ğŸ¯ **88.76% accuracy** on automated decisions
- ğŸ“Š **Full explainability** (SHAP feature importance)

### Technical Achievement:
- ğŸ”¬ Advanced ML pipeline (preprocessing â†’ ensemble â†’ escalation)
- ğŸ“ˆ Uncertainty quantification (validated with correlation)
- ğŸ›ï¸ Hyperparameter optimization (GridSearchCV)
- ğŸ§® Cost-benefit optimization (225 configurations tested)
- ğŸ” Model interpretability (SHAP waterfall plots)

**You've just built a production-grade AI system!** ğŸš€

---

**Questions?** Check `PROGRESS.md` for detailed results or `FINAL_PROJECT_REPORT.md` for explanations.

**Ready to deploy?** See the "Production Deployment" section in the final report.

**Happy coding!** ğŸ’»âœ¨
