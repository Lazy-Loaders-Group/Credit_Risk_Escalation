{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9908443",
   "metadata": {},
   "source": [
    "# Phase 5: Comprehensive Evaluation & Interpretability\n",
    "\n",
    "**Objective:** Complete system evaluation with interpretability analysis\n",
    "\n",
    "**Goals:**\n",
    "- Evaluate complete end-to-end system\n",
    "- Analyze feature importance with SHAP\n",
    "- Perform ablation studies\n",
    "- Generate comprehensive visualizations\n",
    "- Document key insights and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bc29a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import sys\n",
    "import shap\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c50a07",
   "metadata": {},
   "source": [
    "## 2. Load All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c000637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data and models...\")\n",
    "X_train = joblib.load('../data/splits/X_train.pkl')\n",
    "X_test = joblib.load('../data/splits/X_test.pkl')\n",
    "y_test = joblib.load('../data/splits/y_test.pkl')\n",
    "\n",
    "# Load models\n",
    "bootstrap_ensemble = joblib.load('../results/models/bootstrap_ensemble.pkl')\n",
    "escalation_system = joblib.load('../results/models/escalation_system.pkl')\n",
    "preprocessor = joblib.load('../results/models/preprocessor.pkl')\n",
    "\n",
    "# Load uncertainty estimates\n",
    "uncertainty_data = joblib.load('../results/models/uncertainty_estimates.pkl')\n",
    "proba_test = uncertainty_data['test']['proba']\n",
    "uncertainty_test = uncertainty_data['test']['uncertainty']\n",
    "y_pred_test = uncertainty_data['test']['y_pred']\n",
    "\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"Features: {len(preprocessor.feature_names)}\")\n",
    "print(\"âœ… All components loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca3d09",
   "metadata": {},
   "source": [
    "## 3. Complete System Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate complete system\n",
    "print(\"Evaluating complete credit risk assessment system...\\n\")\n",
    "\n",
    "# Get escalation decisions\n",
    "escalate_mask = escalation_system.process_predictions(proba_test, uncertainty_test)\n",
    "\n",
    "# Automated decisions\n",
    "automated_mask = ~escalate_mask\n",
    "n_automated = np.sum(automated_mask)\n",
    "n_escalated = np.sum(escalate_mask)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPLETE SYSTEM PERFORMANCE (TEST SET)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDecision Distribution:\")\n",
    "print(f\"  Total Samples:        {len(y_test)}\")\n",
    "print(f\"  Automated:            {n_automated} ({n_automated/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Escalated to Human:   {n_escalated} ({n_escalated/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Automated performance\n",
    "if n_automated > 0:\n",
    "    y_test_auto = y_test.values[automated_mask]\n",
    "    y_pred_auto = y_pred_test[automated_mask]\n",
    "    proba_auto = proba_test[automated_mask, 1]\n",
    "    \n",
    "    print(f\"\\nAutomated Decisions Performance:\")\n",
    "    print(f\"  Accuracy:             {accuracy_score(y_test_auto, y_pred_auto):.4f}\")\n",
    "    print(f\"  Precision:            {precision_score(y_test_auto, y_pred_auto):.4f}\")\n",
    "    print(f\"  Recall:               {recall_score(y_test_auto, y_pred_auto):.4f}\")\n",
    "    print(f\"  F1-Score:             {f1_score(y_test_auto, y_pred_auto):.4f}\")\n",
    "    print(f\"  AUC-ROC:              {roc_auc_score(y_test_auto, proba_auto):.4f}\")\n",
    "\n",
    "# Escalated cases analysis\n",
    "if n_escalated > 0:\n",
    "    y_test_esc = y_test.values[escalate_mask]\n",
    "    y_pred_esc = y_pred_test[escalate_mask]\n",
    "    \n",
    "    # What would accuracy be if we automated these?\n",
    "    acc_if_automated = accuracy_score(y_test_esc, y_pred_esc)\n",
    "    \n",
    "    print(f\"\\nEscalated Cases Analysis:\")\n",
    "    print(f\"  Accuracy if Automated: {acc_if_automated:.4f}\")\n",
    "    print(f\"  Default Rate:          {y_test_esc.mean():.2%}\")\n",
    "    print(f\"  Avg Uncertainty:       {uncertainty_test[escalate_mask].mean():.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9e9aa",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13751845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive confusion matrix visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Overall confusion matrix\n",
    "cm_overall = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Paid', 'Default'],\n",
    "            yticklabels=['Paid', 'Default'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Overall Predictions\\n(Before Escalation)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontweight='bold')\n",
    "\n",
    "# Automated decisions only\n",
    "if n_automated > 0:\n",
    "    cm_auto = confusion_matrix(y_test_auto, y_pred_auto)\n",
    "    sns.heatmap(cm_auto, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "                xticklabels=['Paid', 'Default'],\n",
    "                yticklabels=['Paid', 'Default'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    axes[1].set_title(f'Automated Decisions Only\\n({n_automated} samples)', \n",
    "                     fontweight='bold', fontsize=12)\n",
    "    axes[1].set_ylabel('True Label', fontweight='bold')\n",
    "    axes[1].set_xlabel('Predicted Label', fontweight='bold')\n",
    "\n",
    "# Escalated cases\n",
    "if n_escalated > 0:\n",
    "    cm_esc = confusion_matrix(y_test_esc, y_pred_esc)\n",
    "    sns.heatmap(cm_esc, annot=True, fmt='d', cmap='Oranges', ax=axes[2],\n",
    "                xticklabels=['Paid', 'Default'],\n",
    "                yticklabels=['Paid', 'Default'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    axes[2].set_title(f'Escalated to Human\\n({n_escalated} samples)',\n",
    "                     fontweight='bold', fontsize=12)\n",
    "    axes[2].set_ylabel('True Label', fontweight='bold')\n",
    "    axes[2].set_xlabel('Predicted Label (if automated)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/final_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Confusion matrices saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f9c52",
   "metadata": {},
   "source": [
    "## 5. SHAP Interpretability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde97f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a base model from the ensemble for SHAP analysis\n",
    "print(\"Performing SHAP analysis...\")\n",
    "print(\"Note: Using first model from bootstrap ensemble for efficiency\\n\")\n",
    "\n",
    "base_model = bootstrap_ensemble.models[0]\n",
    "\n",
    "# Sample data for SHAP (to speed up computation)\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test), min(1000, len(X_test)), replace=False)\n",
    "X_sample = X_test.iloc[sample_indices]\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(base_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(\"âœ… SHAP values calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (Feature Importance)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/shap_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… SHAP importance plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (Impact Direction)\n",
    "plt.figure(figsize=(10, 10))\n",
    "shap.summary_plot(shap_values, X_sample, show=False, max_display=20)\n",
    "plt.title('SHAP Feature Impact Analysis', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… SHAP summary plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cae52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence Plots for top 3 features\n",
    "# Get feature importance\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "top_features_idx = np.argsort(feature_importance)[-3:][::-1]\n",
    "top_features = [X_sample.columns[i] for i in top_features_idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "for idx, (feat_idx, feat_name) in enumerate(zip(top_features_idx, top_features)):\n",
    "    shap.dependence_plot(\n",
    "        feat_idx, \n",
    "        shap_values, \n",
    "        X_sample,\n",
    "        ax=axes[idx],\n",
    "        show=False\n",
    "    )\n",
    "    axes[idx].set_title(f'SHAP Dependence: {feat_name}', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/shap_dependence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… SHAP dependence plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37f23b",
   "metadata": {},
   "source": [
    "## 6. Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fed37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation study: Compare different configurations\n",
    "print(\"Performing ablation study...\\n\")\n",
    "\n",
    "# Load baseline model for comparison\n",
    "baseline_model = joblib.load('../results/models/xgboost_best.pkl')\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "proba_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "# 1. Baseline (single model, no uncertainty, no escalation)\n",
    "ablation_results.append({\n",
    "    'Configuration': 'Baseline (Single Model)',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1-Score': f1_score(y_test, y_pred_baseline),\n",
    "    'AUC-ROC': roc_auc_score(y_test, proba_baseline),\n",
    "    'Automation Rate': 1.0,\n",
    "    'Escalation Rate': 0.0\n",
    "})\n",
    "\n",
    "# 2. Bootstrap Ensemble (no escalation)\n",
    "ablation_results.append({\n",
    "    'Configuration': 'Bootstrap Ensemble Only',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_test),\n",
    "    'Precision': precision_score(y_test, y_pred_test),\n",
    "    'Recall': recall_score(y_test, y_pred_test),\n",
    "    'F1-Score': f1_score(y_test, y_pred_test),\n",
    "    'AUC-ROC': roc_auc_score(y_test, proba_test[:, 1]),\n",
    "    'Automation Rate': 1.0,\n",
    "    'Escalation Rate': 0.0\n",
    "})\n",
    "\n",
    "# 3. Complete System (ensemble + escalation)\n",
    "if n_automated > 0:\n",
    "    ablation_results.append({\n",
    "        'Configuration': 'Complete System (Ensemble + Escalation)',\n",
    "        'Accuracy': accuracy_score(y_test_auto, y_pred_auto),\n",
    "        'Precision': precision_score(y_test_auto, y_pred_auto),\n",
    "        'Recall': recall_score(y_test_auto, y_pred_auto),\n",
    "        'F1-Score': f1_score(y_test_auto, y_pred_auto),\n",
    "        'AUC-ROC': roc_auc_score(y_test_auto, proba_auto),\n",
    "        'Automation Rate': n_automated / len(y_test),\n",
    "        'Escalation Rate': n_escalated / len(y_test)\n",
    "    })\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(ablation_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation study\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = ablation_df[metric].values\n",
    "    bars = ax.bar(range(len(ablation_df)), values, color=colors)\n",
    "    ax.set_xticks(range(len(ablation_df)))\n",
    "    ax.set_xticklabels(ablation_df['Configuration'], rotation=15, ha='right')\n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([min(values) - 0.02, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ablation_study.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Ablation study visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15743334",
   "metadata": {},
   "source": [
    "## 7. Final ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21282056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all configurations\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Baseline\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, proba_baseline)\n",
    "auc_base = roc_auc_score(y_test, proba_baseline)\n",
    "plt.plot(fpr_base, tpr_base, label=f'Baseline Model (AUC={auc_base:.3f})', \n",
    "         linewidth=2, color='#3498db')\n",
    "\n",
    "# Bootstrap Ensemble\n",
    "fpr_ens, tpr_ens, _ = roc_curve(y_test, proba_test[:, 1])\n",
    "auc_ens = roc_auc_score(y_test, proba_test[:, 1])\n",
    "plt.plot(fpr_ens, tpr_ens, label=f'Bootstrap Ensemble (AUC={auc_ens:.3f})',\n",
    "         linewidth=2, color='#2ecc71')\n",
    "\n",
    "# Complete System (automated only)\n",
    "if n_automated > 0:\n",
    "    fpr_auto, tpr_auto, _ = roc_curve(y_test_auto, proba_auto)\n",
    "    auc_auto = roc_auc_score(y_test_auto, proba_auto)\n",
    "    plt.plot(fpr_auto, tpr_auto, \n",
    "             label=f'Complete System - Automated ({n_automated} samples, AUC={auc_auto:.3f})',\n",
    "             linewidth=2, color='#e74c3c')\n",
    "\n",
    "# Random baseline\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - System Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/final_roc_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Final ROC comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87c122",
   "metadata": {},
   "source": [
    "## 8. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate business impact\n",
    "print(\"\\nBusiness Impact Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Costs from escalation system\n",
    "costs = escalation_system.calculate_costs(y_test.values, y_pred_test, escalate_mask)\n",
    "\n",
    "print(f\"\\nCost Analysis (Test Set):\")\n",
    "print(f\"  Baseline Total Cost:      ${costs['baseline_cost']:.2f}\")\n",
    "print(f\"  System Total Cost:        ${costs['total_cost']:.2f}\")\n",
    "print(f\"  Cost Savings:             ${costs['cost_savings']:.2f}\")\n",
    "print(f\"  Savings Rate:             {costs['cost_savings']/costs['baseline_cost']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nCost Breakdown:\")\n",
    "print(f\"  False Positive Cost:      ${costs['false_positive_cost']:.2f}\")\n",
    "print(f\"  False Negative Cost:      ${costs['false_negative_cost']:.2f}\")\n",
    "print(f\"  Human Review Cost:        ${costs['escalation_cost']:.2f}\")\n",
    "\n",
    "print(f\"\\nOperational Metrics:\")\n",
    "print(f\"  Applications Processed:   {len(y_test)}\")\n",
    "print(f\"  Automated Decisions:      {n_automated} ({n_automated/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Human Reviews Required:   {n_escalated} ({n_escalated/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Efficiency gain\n",
    "time_per_manual_review = 15  # minutes\n",
    "time_saved = n_automated * time_per_manual_review / 60  # hours\n",
    "print(f\"\\nTime Efficiency:\")\n",
    "print(f\"  Time Saved (vs manual):   {time_saved:.1f} hours\")\n",
    "print(f\"  Productivity Gain:        {n_automated/len(y_test)*100:.1f}%\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize business impact\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cost comparison\n",
    "costs_data = ['Baseline', 'With System']\n",
    "costs_values = [costs['baseline_cost'], costs['total_cost']]\n",
    "colors_cost = ['#e74c3c', '#2ecc71']\n",
    "bars = axes[0, 0].bar(costs_data, costs_values, color=colors_cost)\n",
    "axes[0, 0].set_ylabel('Total Cost ($)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Total Cost Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'${height:.2f}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# Cost breakdown\n",
    "cost_breakdown = {\n",
    "    'False\\nPositives': costs['false_positive_cost'],\n",
    "    'False\\nNegatives': costs['false_negative_cost'],\n",
    "    'Human\\nReview': costs['escalation_cost']\n",
    "}\n",
    "axes[0, 1].bar(cost_breakdown.keys(), cost_breakdown.values(), \n",
    "               color=['#e74c3c', '#f39c12', '#3498db'])\n",
    "axes[0, 1].set_ylabel('Cost ($)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Cost Breakdown', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Decision distribution\n",
    "decision_data = ['Automated', 'Escalated']\n",
    "decision_values = [n_automated, n_escalated]\n",
    "explode = (0.05, 0)\n",
    "axes[1, 0].pie(decision_values, labels=decision_data, autopct='%1.1f%%',\n",
    "               startangle=90, colors=['#2ecc71', '#e74c3c'], explode=explode)\n",
    "axes[1, 0].set_title('Decision Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Accuracy by category\n",
    "acc_data = {\n",
    "    'Baseline\\nModel': ablation_df.loc[0, 'Accuracy'],\n",
    "    'Bootstrap\\nEnsemble': ablation_df.loc[1, 'Accuracy'],\n",
    "    'System\\n(Automated)': ablation_df.loc[2, 'Accuracy'] if len(ablation_df) > 2 else 0\n",
    "}\n",
    "bars = axes[1, 1].bar(acc_data.keys(), acc_data.values(), \n",
    "                      color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([min(acc_data.values()) - 0.01, 1.0])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/business_impact.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Business impact visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfb1f9",
   "metadata": {},
   "source": [
    "## 9. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be59bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ablation study results\n",
    "ablation_df.to_csv('../results/reports/ablation_study_results.csv', index=False)\n",
    "print(\"âœ… Ablation study results saved\")\n",
    "\n",
    "# Save business impact summary\n",
    "business_impact = pd.DataFrame([{\n",
    "    'Metric': 'Total Cost Baseline',\n",
    "    'Value': costs['baseline_cost']\n",
    "}, {\n",
    "    'Metric': 'Total Cost with System',\n",
    "    'Value': costs['total_cost']\n",
    "}, {\n",
    "    'Metric': 'Cost Savings',\n",
    "    'Value': costs['cost_savings']\n",
    "}, {\n",
    "    'Metric': 'Savings Rate (%)',\n",
    "    'Value': costs['cost_savings']/costs['baseline_cost']*100\n",
    "}, {\n",
    "    'Metric': 'Automation Rate (%)',\n",
    "    'Value': n_automated/len(y_test)*100\n",
    "}, {\n",
    "    'Metric': 'Escalation Rate (%)',\n",
    "    'Value': n_escalated/len(y_test)*100\n",
    "}])\n",
    "business_impact.to_csv('../results/reports/business_impact_summary.csv', index=False)\n",
    "print(\"âœ… Business impact summary saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e293cb",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 5 COMPLETE: COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâœ… Achievements:\")\n",
    "print(\"   - Complete end-to-end system evaluated\")\n",
    "print(\"   - SHAP interpretability analysis completed\")\n",
    "print(\"   - Ablation study performed\")\n",
    "print(\"   - Business impact quantified\")\n",
    "print(\"   - All visualizations generated\")\n",
    "\n",
    "print(\"\\nðŸ“Š Final System Performance:\")\n",
    "print(f\"   Automation Rate:          {n_automated/len(y_test)*100:.1f}%\")\n",
    "print(f\"   Automated Accuracy:       {accuracy_score(y_test_auto, y_pred_auto):.4f}\")\n",
    "print(f\"   Automated AUC-ROC:        {roc_auc_score(y_test_auto, proba_auto):.4f}\")\n",
    "print(f\"   Cost Savings:             ${costs['cost_savings']:.2f}\")\n",
    "print(f\"   Savings Rate:             {costs['cost_savings']/costs['baseline_cost']*100:.1f}%\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Project Success Criteria:\")\n",
    "if n_automated/len(y_test) >= 0.70:\n",
    "    print(f\"   âœ… Automation â‰¥70%: {n_automated/len(y_test)*100:.1f}%\")\n",
    "if accuracy_score(y_test_auto, y_pred_auto) >= 0.85:\n",
    "    print(f\"   âœ… Accuracy â‰¥85%: {accuracy_score(y_test_auto, y_pred_auto):.4f}\")\n",
    "if costs['cost_savings'] > 0:\n",
    "    print(f\"   âœ… Positive cost savings: ${costs['cost_savings']:.2f}\")\n",
    "\n",
    "print(\"\\nðŸš€ Next: Phase 6 - Final Documentation\")\n",
    "print(\"   - Comprehensive project report\")\n",
    "print(\"   - Presentation slides\")\n",
    "print(\"   - Code documentation\")\n",
    "print(\"   - README and deployment guide\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
