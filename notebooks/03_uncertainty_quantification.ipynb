{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd60954b",
   "metadata": {},
   "source": [
    "# Phase 3: Uncertainty Quantification\n",
    "\n",
    "**Objective:** Implement and validate uncertainty quantification methods\n",
    "\n",
    "**Goals:**\n",
    "- Implement Bootstrap Ensemble for uncertainty estimation\n",
    "- Validate uncertainty quality (correlation with errors)\n",
    "- Compare with baseline model\n",
    "- Analyze calibration\n",
    "- Prepare for escalation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71566324",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Custom modules\n",
    "from uncertainty_quantification import (\n",
    "    BootstrapEnsemble,\n",
    "    UncertaintyMetrics,\n",
    "    TemperatureScaling,\n",
    "    analyze_uncertainty_quality\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eda29d",
   "metadata": {},
   "source": [
    "## 2. Load Data and Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd776d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "print(\"Loading data splits...\")\n",
    "X_train = joblib.load('../data/splits/X_train.pkl')\n",
    "X_val = joblib.load('../data/splits/X_val.pkl')\n",
    "X_test = joblib.load('../data/splits/X_test.pkl')\n",
    "y_train = joblib.load('../data/splits/y_train.pkl')\n",
    "y_val = joblib.load('../data/splits/y_val.pkl')\n",
    "y_test = joblib.load('../data/splits/y_test.pkl')\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Val shape: {X_val.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model from Phase 2\n",
    "print(\"\\nLoading best model...\")\n",
    "best_model = joblib.load('../results/models/xgboost_best.pkl')\n",
    "print(\"‚úÖ XGBoost model loaded\")\n",
    "\n",
    "# Quick baseline evaluation\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"\\nBaseline Performance (Validation):\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"  AUC-ROC:  {roc_auc_score(y_val, y_val_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d473843",
   "metadata": {},
   "source": [
    "## 3. Build Bootstrap Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bootstrap Ensemble\n",
    "print(\"Creating Bootstrap Ensemble...\")\n",
    "print(\"This will train 30 models on bootstrapped samples.\")\n",
    "print(\"Estimated time: 5-10 minutes\\n\")\n",
    "\n",
    "bootstrap_ensemble = BootstrapEnsemble(\n",
    "    base_model=best_model,\n",
    "    n_estimators=30,\n",
    "    bootstrap_size=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train ensemble\n",
    "bootstrap_ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994b685",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3489b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set predictions\n",
    "print(\"Generating predictions with uncertainty estimates...\")\n",
    "proba_val, uncertainty_val, all_proba_val = bootstrap_ensemble.predict_with_uncertainty(X_val)\n",
    "y_val_pred_ens = (proba_val[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nEnsemble Performance (Validation):\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_val, y_val_pred_ens):.4f}\")\n",
    "print(f\"  AUC-ROC:  {roc_auc_score(y_val, proba_val[:, 1]):.4f}\")\n",
    "\n",
    "print(f\"\\nUncertainty Statistics:\")\n",
    "print(f\"  Mean: {np.mean(uncertainty_val):.4f}\")\n",
    "print(f\"  Std:  {np.std(uncertainty_val):.4f}\")\n",
    "print(f\"  Min:  {np.min(uncertainty_val):.4f}\")\n",
    "print(f\"  Max:  {np.max(uncertainty_val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c299b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predictions\n",
    "print(\"\\nGenerating test set predictions...\")\n",
    "proba_test, uncertainty_test, all_proba_test = bootstrap_ensemble.predict_with_uncertainty(X_test)\n",
    "y_test_pred_ens = (proba_test[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nEnsemble Performance (Test):\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_test_pred_ens):.4f}\")\n",
    "print(f\"  AUC-ROC:  {roc_auc_score(y_test, proba_test[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c9572",
   "metadata": {},
   "source": [
    "## 5. Analyze Uncertainty Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ad6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive uncertainty analysis\n",
    "results_val = analyze_uncertainty_quality(\n",
    "    y_val,\n",
    "    y_val_pred_ens,\n",
    "    proba_val[:, 1],\n",
    "    uncertainty_val,\n",
    "    title=\"Validation Set Uncertainty Analysis\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + results_val.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set analysis\n",
    "results_test = analyze_uncertainty_quality(\n",
    "    y_test,\n",
    "    y_test_pred_ens,\n",
    "    proba_test[:, 1],\n",
    "    uncertainty_test,\n",
    "    title=\"Test Set Uncertainty Analysis\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + results_test.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe0e52",
   "metadata": {},
   "source": [
    "## 6. Visualize Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(uncertainty_val, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(np.mean(uncertainty_val), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {np.mean(uncertainty_val):.4f}')\n",
    "axes[0].set_xlabel('Uncertainty', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Uncertainty Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot by correctness\n",
    "correct = (y_val == y_val_pred_ens)\n",
    "data_box = pd.DataFrame({\n",
    "    'Uncertainty': uncertainty_val,\n",
    "    'Prediction': ['Correct' if c else 'Incorrect' for c in correct]\n",
    "})\n",
    "\n",
    "sns.boxplot(data=data_box, x='Prediction', y='Uncertainty', ax=axes[1],\n",
    "            palette={'Correct': 'lightgreen', 'Incorrect': 'lightcoral'})\n",
    "axes[1].set_title('Uncertainty by Prediction Correctness', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Uncertainty', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/uncertainty_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Uncertainty distribution saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty vs Probability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "colors = ['green' if c else 'red' for c in correct]\n",
    "axes[0].scatter(proba_val[:, 1], uncertainty_val, c=colors, alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('Predicted Probability (Default)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Uncertainty', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Uncertainty vs Predicted Probability', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(0.5, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', alpha=0.5, label='Correct'),\n",
    "    Patch(facecolor='red', alpha=0.5, label='Incorrect')\n",
    "]\n",
    "axes[0].legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Binned uncertainty by probability\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_indices = np.digitize(proba_val[:, 1], bins) - 1\n",
    "bin_uncertainty = [uncertainty_val[bin_indices == i].mean() \n",
    "                   for i in range(len(bins) - 1)]\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "axes[1].plot(bin_centers, bin_uncertainty, 'o-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Predicted Probability (Default)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Average Uncertainty', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Average Uncertainty by Probability Bin', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/uncertainty_vs_probability.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Uncertainty vs probability plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0cb20",
   "metadata": {},
   "source": [
    "## 7. Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c75231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction intervals\n",
    "print(\"Calculating prediction intervals...\")\n",
    "proba_mean, lower_bound, upper_bound = bootstrap_ensemble.get_prediction_intervals(\n",
    "    X_val, confidence=0.95\n",
    ")\n",
    "\n",
    "# Plot prediction intervals for subset\n",
    "n_samples = 100\n",
    "indices = np.random.choice(len(X_val), n_samples, replace=False)\n",
    "indices = np.sort(indices)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "x = np.arange(n_samples)\n",
    "\n",
    "# True values\n",
    "plt.scatter(x, y_val.values[indices], color='black', s=30, \n",
    "            label='True Value', zorder=3, alpha=0.6)\n",
    "\n",
    "# Predictions with intervals\n",
    "plt.plot(x, proba_mean[indices], 'o-', color='blue', \n",
    "         label='Predicted Probability', markersize=4, alpha=0.7)\n",
    "plt.fill_between(x, lower_bound[indices], upper_bound[indices], \n",
    "                 color='lightblue', alpha=0.5, label='95% Confidence Interval')\n",
    "\n",
    "plt.xlabel('Sample Index', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Probability / True Value', fontsize=12, fontweight='bold')\n",
    "plt.title('Prediction Intervals (95% Confidence)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/prediction_intervals.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Prediction intervals plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf356d",
   "metadata": {},
   "source": [
    "## 8. Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare calibration: baseline vs ensemble\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Baseline model calibration\n",
    "prob_true_base, prob_pred_base = calibration_curve(\n",
    "    y_val, y_val_proba, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "axes[0].plot(prob_pred_base, prob_true_base, 's-', \n",
    "             label='Baseline XGBoost', markersize=8, linewidth=2)\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=1)\n",
    "axes[0].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Probability', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Baseline Model Calibration', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='best', fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Ensemble calibration\n",
    "prob_true_ens, prob_pred_ens = calibration_curve(\n",
    "    y_val, proba_val[:, 1], n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "axes[1].plot(prob_pred_ens, prob_true_ens, 'o-', \n",
    "             label='Bootstrap Ensemble', markersize=8, linewidth=2, color='green')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=1)\n",
    "axes[1].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True Probability', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Bootstrap Ensemble Calibration', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='best', fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/calibration_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Calibration comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb189a",
   "metadata": {},
   "source": [
    "## 9. Uncertainty by Risk Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize by predicted risk level\n",
    "risk_categories = pd.cut(\n",
    "    proba_val[:, 1],\n",
    "    bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "# Analyze uncertainty by risk level\n",
    "uncertainty_by_risk = pd.DataFrame({\n",
    "    'Risk Level': risk_categories,\n",
    "    'Uncertainty': uncertainty_val,\n",
    "    'True Default': y_val.values\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=uncertainty_by_risk, x='Risk Level', y='Uncertainty', ax=axes[0],\n",
    "            palette='RdYlGn_r')\n",
    "axes[0].set_title('Uncertainty by Risk Level', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Risk Level', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Uncertainty', fontsize=12, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Summary statistics\n",
    "summary = uncertainty_by_risk.groupby('Risk Level').agg({\n",
    "    'Uncertainty': ['mean', 'std', 'count'],\n",
    "    'True Default': 'mean'\n",
    "}).round(4)\n",
    "summary.columns = ['Avg Uncertainty', 'Std Uncertainty', 'Count', 'Default Rate']\n",
    "print(\"\\nUncertainty by Risk Level:\")\n",
    "print(summary)\n",
    "\n",
    "# Heatmap\n",
    "pivot_data = uncertainty_by_risk.groupby('Risk Level')['Uncertainty'].mean()\n",
    "axes[1].bar(range(len(pivot_data)), pivot_data.values, color='coral')\n",
    "axes[1].set_xticks(range(len(pivot_data)))\n",
    "axes[1].set_xticklabels(pivot_data.index, rotation=45)\n",
    "axes[1].set_ylabel('Average Uncertainty', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Risk Level', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Average Uncertainty by Risk Level', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/uncertainty_by_risk.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Uncertainty by risk level analysis saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599f2eb",
   "metadata": {},
   "source": [
    "## 10. Save Bootstrap Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45292a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save bootstrap ensemble\n",
    "print(\"Saving bootstrap ensemble...\")\n",
    "joblib.dump(bootstrap_ensemble, '../results/models/bootstrap_ensemble.pkl')\n",
    "print(\"‚úÖ Bootstrap ensemble saved\")\n",
    "\n",
    "# Save uncertainty estimates for later use\n",
    "uncertainty_data = {\n",
    "    'val': {\n",
    "        'proba': proba_val,\n",
    "        'uncertainty': uncertainty_val,\n",
    "        'y_true': y_val.values,\n",
    "        'y_pred': y_val_pred_ens\n",
    "    },\n",
    "    'test': {\n",
    "        'proba': proba_test,\n",
    "        'uncertainty': uncertainty_test,\n",
    "        'y_true': y_test.values,\n",
    "        'y_pred': y_test_pred_ens\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(uncertainty_data, '../results/models/uncertainty_estimates.pkl')\n",
    "print(\"‚úÖ Uncertainty estimates saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5383736",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 3 COMPLETE: UNCERTAINTY QUANTIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ Achievements:\")\n",
    "print(\"   - Bootstrap Ensemble with 30 models trained\")\n",
    "print(\"   - Uncertainty estimates generated for all predictions\")\n",
    "print(\"   - Uncertainty quality validated (correlation with errors)\")\n",
    "print(\"   - Calibration analyzed and compared\")\n",
    "print(\"   - Prediction intervals calculated\")\n",
    "\n",
    "print(\"\\nüìä Key Metrics (Validation Set):\")\n",
    "print(f\"   Ensemble Accuracy: {accuracy_score(y_val, y_val_pred_ens):.4f}\")\n",
    "print(f\"   Ensemble AUC-ROC:  {roc_auc_score(y_val, proba_val[:, 1]):.4f}\")\n",
    "print(f\"   Mean Uncertainty:  {np.mean(uncertainty_val):.4f}\")\n",
    "\n",
    "# Calculate correlation\n",
    "errors = (y_val != y_val_pred_ens).astype(int)\n",
    "correlation = np.corrcoef(uncertainty_val, errors)[0, 1]\n",
    "print(f\"   Uncertainty-Error Correlation: {correlation:.4f}\")\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = UncertaintyMetrics()\n",
    "corr_results = metrics.uncertainty_vs_error_correlation(\n",
    "    y_val.values, y_val_pred_ens, uncertainty_val\n",
    ")\n",
    "print(f\"   Uncertainty Ratio (Incorrect/Correct): {corr_results['uncertainty_ratio']:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ Phase 3 Success Criteria:\")\n",
    "if correlation > 0.3:\n",
    "    print(f\"   ‚úÖ Strong correlation (>0.3): {correlation:.4f}\")\n",
    "elif correlation > 0.15:\n",
    "    print(f\"   ‚úÖ Moderate correlation (>0.15): {correlation:.4f}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Weak correlation: {correlation:.4f}\")\n",
    "\n",
    "if corr_results['uncertainty_ratio'] > 1.5:\n",
    "    print(f\"   ‚úÖ Uncertainty ratio >1.5: {corr_results['uncertainty_ratio']:.4f}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Uncertainty ratio: {corr_results['uncertainty_ratio']:.4f}\")\n",
    "\n",
    "print(\"\\nüöÄ Next: Phase 4 - Human Escalation System\")\n",
    "print(\"   - Define escalation criteria based on uncertainty\")\n",
    "print(\"   - Optimize thresholds for cost-benefit\")\n",
    "print(\"   - Implement EscalationSystem class\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
